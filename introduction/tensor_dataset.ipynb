{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with exploring different options for torch installations. There are mainly two things to consider: your operating system, and your graphic card. Torch makes available CPU-only versions and CUDA-based versions. CUDA is an API (application programming interface), a software layer for NVIDIA graphic cards that gives direct access to your GPU device and also manages numerous concepts such a parallelization and convolution algorithms. ROCm is also a similar software, but for AMD graphic cards. To train models on your GPU, you need a CUDA-based or ROC-based torch installation.\n",
    "\n",
    "- __For Macbook owners__: Recently torch announced there will be support for Macbook chips with the new Metal Performance Shaders (MPS) backend for GPU accelaration. You can install a default version of torch, or also use Google Colab.\n",
    "\n",
    "- __For Windows users__: You need to have an NVIDIA graphic card, compatible with the CUDA-based torch instalations of your preferred torch version. If you have an old graphic card (older than ~3-4 years), you might need to install an older version of torch (check previous versions). If you have an AMD graphic card, you can install WSL (Windows Subsystem of Linux) and pick a ROCm-based torch installation. In any case, I would highly recommend WSL which might increase your options and easier to manage.\n",
    "\n",
    "- __For Linux or WSL users__: If you use a Linux-based system, you can either use a AMD or NVIDIA graphics card.\n",
    "\n",
    "For installation options, Torch website is also quite explanatory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![installation.png](./installation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Important Note__: if your PC does not have a GPU and compatible CUDA compiler installation (usually comes with NVIDIA drivers), you can run this notebook on Google Colab. It also should have its own torch installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, this notebook is prepared on WSL and torch version 2.1.1 with CUDA 11.8. The device is NVIDIA RTX 3060 which is only compatible with CUDA versions >10. So the installation command would be (Check torch website to find the command for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other packages that will be used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor # Only for type-annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "- https://pytorch.org/\n",
    "\n",
    "- https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Torch Tensor objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1365, 1.1233, 1.8108],\n",
       "        [1.9765, 1.5564, 1.1649]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor = torch.rand(2, 3, requires_grad=True)\n",
    "a_tensor + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something different than the regular numpy arrays? Tensors used in training should be attached to gradient function (grad_fn) that is responsible for each of the backward calculations (i.e., backpropagation steps). That will be important in the training (Section 3). These gradient functions basically used to keep track of whatever happens to the tensors and that creates a tree-like structure, and this tree is called 'gradient tree'. Something that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradient_tree.png](./gradient_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Create 2 tensors a and b with size [3, 5] both, where all b's are 8 and all a's are 2. You can use \"number * torch.ones()\" (Check torch.ones documentation for details). Then calculate the following formula: \n",
    "\n",
    "$$ \\frac{b + \\sqrt{b^2 - 4a}}{2a}$$\n",
    "\n",
    "Track and list gradient functions step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7f49e0f2dba0>\n",
      "<MulBackward0 object at 0x7f4abb11f7f0>\n",
      "<SubBackward0 object at 0x7f4abb11f7f0>\n",
      "<SqrtBackward0 object at 0x7f4abb11f7f0>\n",
      "<AddBackward0 object at 0x7f4abb11f7f0>\n",
      "<MulBackward0 object at 0x7f4abb11f7f0>\n",
      "<DivBackward0 object at 0x7f4abb11f7f0>\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working with GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used property of the torch tensors is that you can move them between GPU and CPU memory ! Lets first check if your torch installation is ready-to-use with GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # ROCm also translated to CUDA.\n",
    "# torch.backends.mps.is_available() # This line is for MAC users with MPS based torch installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error on that line, that would probably because of your torch installation is not a CUDA-supported installation. You can replace \"cuda\" with \"backends.mps\". If the above line returned True, you can also run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5236, 0.5454, 0.7520],\n",
       "        [0.4329, 0.9686, 0.4871]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "a_tensor = torch.rand(2, 3, requires_grad=True, device=device)\n",
    "a_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we created a tensor directly in your GPU memory. But also, we can move it to CPU memory (i.e., RAM) and again back to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5236, 0.5454, 0.7520],\n",
      "        [0.4329, 0.9686, 0.4871]], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[0.5236, 0.5454, 0.7520],\n",
      "        [0.4329, 0.9686, 0.4871]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a_tensor = a_tensor.cpu()\n",
    "print(a_tensor)\n",
    "b_tensor = a_tensor.to(device)\n",
    "print(b_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Create two tensors: x_tensor and y_tensor with size [100,1000] both. Allocate x_tensor on GPU, and y_tensor on CPU. Then calculate this: \n",
    "\n",
    "$$ \\frac{x^2}{2x}$$\n",
    "\n",
    " for both tensors separately and report execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.4 µs ± 534 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# You can use %timeit for whole operation to measure time.\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9 µs ± 523 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Better to use %timeit in separate cells.\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: GPU accelaration is usually more effective on backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Torch Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class is the backbone of the data operations of any kind. Usually it is a good idea to create your own dataset class to have dataset-specific functionalities. Such as preprocessing, augmentations, IO-operations, tensor shaping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "\n",
    "# Built-in Dataset class should be inherited from your custom datasets.\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, path_to_data: str) -> None:\n",
    "        # Initialize the inherited class with super().\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_data = path_to_data\n",
    "        self.images_list = os.listdir(os.path.join(self.path_to_data, \"images\"))\n",
    "        self.metadata = pd.read_csv(os.path.join(self.path_to_data, \"pokemon.csv\"))\n",
    "        # Create label encoding for the text labels to make them machine-understandable.\n",
    "        self.metadata[\"label_code\"] = pd.Categorical(self.metadata[\"Type1\"]).codes\n",
    "\n",
    "    def __getitem__(self, idx: Tensor) -> Tensor:\n",
    "        \"\"\"Compulsory for iteration.\"\"\"\n",
    "        # Get the next image file name from the list.\n",
    "        image_name = self.images_list[idx]\n",
    "        # Create the path to image.\n",
    "        path_to_image = os.path.join(self.path_to_data, \"images\", image_name)\n",
    "        # Read image to a torch tensor. Since we are creating a new tensor here, we should move it to our preferred device.\n",
    "        image = read_image(path_to_image).to(device)\n",
    "        # Apply transformations.\n",
    "        transformed_image = self.augmentation(self.preprocessing(image))\n",
    "        # Pokemon name is the file name but without the extension .png, .jpg etc.\n",
    "        pokemon_name = image_name.split(\".\")[0]\n",
    "        # Find the pokemon from the csv and get its label code.\n",
    "        label = self.metadata[self.metadata[\"Name\"] == pokemon_name][\"label_code\"].to_numpy()\n",
    "        # Return. Since we are creating a tensor here (torch.from_numpy), we should move it to our preferred device.\n",
    "        return transformed_image, torch.from_numpy(label).to(device).squeeze()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compulsory for iteration.\"\"\"\n",
    "        return len(self.images_list)\n",
    "    \n",
    "    # You can create other functions to simplify your code (__getitem__ function), such as get_label() or get_image() etc.\n",
    "    \n",
    "    def preprocessing(self, img: Tensor) -> Tensor:\n",
    "        # Optional\n",
    "        return img\n",
    "    \n",
    "    def augmentation(self, img: Tensor) -> Tensor:\n",
    "        # Optional\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Torch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader is a class that helps to iterate over your dataset, handles batching, data parallelization and various other functionalities. It has a default way to create batches, but needs an input Dataset class to iterate over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collating is a term that its functionality collects multiple samples (and its labels) to a batch and defines the batch-level Dataloader behaviour. Basically you can create a function that accepts batch_data (list of whatever is returned from __getitem function of your Dataset class). In this example, it is list of tuple of 2 tensors, these Tensors are the ones returned as (images, labels) from the __getitem function above. And the list of these actually multiple samples that you should create the batch from. Length of this list should be equal to your BATCH_SIZE. Collate function is usually provide more flexibility, especially when we need batches with different size each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate\n",
    "from typing import List, Tuple # for type-annotations\n",
    "\n",
    "def my_batch_collate(batch_data: List[Tuple[Tensor, Tensor]]) -> Tensor:\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "PATH_TO_DATA = \"../sample_data/\"\n",
    "\n",
    "tr_dataset = PokemonDataset(PATH_TO_DATA)\n",
    "tr_dataloader = DataLoader(\n",
    "    tr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=default_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration should create a tensor of images in the following shape: [5, 4, 120, 120] which represents [batch_size, n_channels, img_height, img_width]. Note that RGB images have 3-channels, RGBA 4-channels and grayscale only one channel. Real data is usually very messy, thats why you will probably get errors most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out how the data is iterated !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 120, 120])\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4, 120, 120] at entry 0 and [3, 120, 120] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m image, label \u001b[39min\u001b[39;00m tr_dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(label\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/GitRepos/basira-deep-learning-tutorials/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 120, 120] at entry 0 and [3, 120, 120] at entry 2"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image, label in tr_dataloader:\n",
    "    print(image.shape)\n",
    "    print(label.shape)\n",
    "\n",
    "    # Plot first image in the batch as grayscale (first channel).\n",
    "    # Note that we need to detach the gradients and move back to CPU to convert tensors to numpy !\n",
    "    plt.imshow(image.detach().cpu().numpy()[0, 0], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: As you have noticed, you cannot create a batch if there are images in the same batch with different color coding. You can handle this situation by padding for RGB and grayscale images to 4-channels, or you can also pick only one channel. This is a good example why we should use our own batch collation function for dataloading, or you can change this in Dataset class as well (suitable case for preprocessing function.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HINT__: Its easier when you use the torch functions: torch.stack() and torch.tensor.expand(). Check documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def img_channel_padding_collate(batch_data: List[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]:\n",
    "    # batch_data is a list of inputs returned from __getitem__ function of the Dataset class.\n",
    "    # Here you need to create one single tensor with size [batch_size, n_channels=4, img_height, img_width].\n",
    "\n",
    "    # Handle the situation here:\n",
    "    for img, label in batch_data:\n",
    "        print(img.shape, label)\n",
    "        \n",
    "    return # Return 2 tensors: batched_images, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader and attach your collate function.\n",
    "tr_dataloader = DataLoader(\n",
    "    tr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=img_channel_padding_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 120, 120]) tensor(13, device='cuda:0', dtype=torch.int8)\n",
      "torch.Size([4, 120, 120]) tensor(16, device='cuda:0', dtype=torch.int8)\n",
      "torch.Size([4, 120, 120]) tensor(3, device='cuda:0', dtype=torch.int8)\n",
      "torch.Size([4, 120, 120]) tensor(0, device='cuda:0', dtype=torch.int8)\n",
      "torch.Size([4, 120, 120]) tensor(8, device='cuda:0', dtype=torch.int8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb Cell 47\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Try to run the iteration again:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m image, label \u001b[39min\u001b[39;00m tr_dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oytun/GitRepos/basira-deep-learning-tutorials/introduction/tensor_dataset.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(label\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Try to run the iteration again:\n",
    "for image, label in tr_dataloader:\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
