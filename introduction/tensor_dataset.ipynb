{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with exploring different options for torch installations. There are mainly two things to consider: your operating system, and your graphic card. Torch makes available CPU-only versions and CUDA-based versions. CUDA is an API (application programming interface), a software layer for NVIDIA graphic cards that gives direct access to your GPU device and also manages numerous concepts such a parallelization and convolution algorithms. ROCm is also a similar software, but for AMD graphic cards. To train models on your GPU, you need a CUDA-based or ROC-based torch installation.\n",
    "\n",
    "- __For Macbook owners__: Recently torch announced there will be support for Macbook chips with the new Metal Performance Shaders (MPS) backend for GPU accelaration. You can install a default version of torch, or also use Google Colab.\n",
    "\n",
    "- __For Windows users__: You need to have an NVIDIA graphic card, compatible with the CUDA-based torch instalations of your preferred torch version. If you have an old graphic card (older than ~3-4 years), you might need to install an older version of torch (check previous versions). If you have an AMD graphic card, you can install WSL (Windows Subsystem of Linux) and pick a ROCm-based torch installation. In any case, I would highly recommend WSL which might increase your options and easier to manage.\n",
    "\n",
    "- __For Linux or WSL users__: If you use a Linux-based system, you can either use a AMD or NVIDIA graphics card.\n",
    "\n",
    "For installation options, Torch website is also quite explanatory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![installation.png](./installation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Important Note__: if your PC does not have a GPU and compatible CUDA compiler installation (usually comes with NVIDIA drivers), you can run this notebook on Google Colab. It also should have its own torch installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, this notebook is prepared on WSL and torch version 2.1.1 with CUDA 11.8. The device is NVIDIA RTX 3060 which is only compatible with CUDA versions >10. So the installation command would be (Check torch website to find the command for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other packages that will be used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor # Only for type-annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "- https://pytorch.org/\n",
    "\n",
    "- https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Torch Tensor objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are multi-dimensional data containers, a generalized concept of matrices to N-dimensional space. Torch uses an object oriented paradigm of \"Tensor\" class to handle data operations including calculations, shaping, automatic differentiation and various other functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key attributes for a torch Tensor object is the attached gradient function. This basically tracks whatever operation is performed on the tensor, and also carried into the other tensors created with the tensor of calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2861, 1.0446, 1.0630],\n",
       "        [1.4396, 1.5288, 1.3056]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to set attribute requires_grad as True to enable gradients.\n",
    "a_tensor = torch.rand(2, 3, requires_grad=True)\n",
    "a_tensor + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5711, 0.5111, 0.5157],\n",
       "        [0.6082, 0.6292, 0.5758]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The gradient function will be transferred into the new tensors you create from the gradient-activated tensors.\n",
    "other_tensor = torch.sigmoid(a_tensor)\n",
    "other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tracking gradients is a costly operation, especially in terms of memory consumption. Therefore, when we do not need the gradients, there is an easy way to disable them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    other_tensor = torch.sigmoid(a_tensor + 1)\n",
    "\n",
    "other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attached gradient functions (grad_fn) are responsible for each of the backward calculations (i.e., backpropagation steps). That will be important in the training (Section 3). When we collect these gradient functions together, they create a tree-like structure, and this tree is called 'gradient tree' as we can see in the example figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradient_tree.png](./gradient_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Create 2 tensors a and b with size [3, 5] both, where all b's are 8 and all a's are 2. You can use \"number * torch.ones()\" (Check torch.ones documentation for details). Then calculate the following formula: \n",
    "\n",
    "$$ \\frac{b + \\sqrt{b^2 - 4a}}{2a}$$\n",
    "\n",
    "Track and list gradient functions step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working with GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used property of the torch tensors is that you can move them between GPU and CPU memory ! Lets first check if your torch installation is ready-to-use with GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() # ROCm also translated to CUDA.\n",
    "# torch.backends.mps.is_available() # This line is for MAC users with MPS based torch installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error on that line, that would probably because of your torch installation is not a CUDA-supported installation. You can replace \"cuda\" with \"backends.mps\". If the above line returned True, you can also run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "a_tensor = torch.rand(2, 3, requires_grad=True, device=device)\n",
    "a_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we created a tensor directly in your GPU memory. But also, we can move it to CPU memory (i.e., RAM) and again back to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor = a_tensor.cpu()\n",
    "print(a_tensor)\n",
    "b_tensor = a_tensor.to(device)\n",
    "print(b_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Create two tensors: x_tensor and y_tensor with size [100,1000] both. Allocate x_tensor on GPU, and y_tensor on CPU. Then calculate this: \n",
    "\n",
    "$$ \\frac{x^2}{2x}$$\n",
    "\n",
    " for both tensors separately and report execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use %timeit for whole operation to measure time.\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to use %timeit in separate cells.\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: GPU accelaration is usually more effective on backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Torch Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class is the backbone of the data operations of any kind. Usually it is a good idea to create your own dataset class to have dataset-specific functionalities. Such as preprocessing, augmentations, IO-operations, tensor shaping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "\n",
    "# Built-in Dataset class should be inherited from your custom datasets.\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, path_to_data: str) -> None:\n",
    "        # Initialize the inherited class with super().\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_data = path_to_data\n",
    "        self.images_list = os.listdir(os.path.join(self.path_to_data, \"images\"))\n",
    "        self.metadata = pd.read_csv(os.path.join(self.path_to_data, \"pokemon.csv\"))\n",
    "        # Create label encoding for the text labels to make them machine-understandable.\n",
    "        self.metadata[\"label_code\"] = pd.Categorical(self.metadata[\"Type1\"]).codes\n",
    "\n",
    "    def __getitem__(self, idx: Tensor) -> Tensor:\n",
    "        \"\"\"Compulsory for iteration.\"\"\"\n",
    "        # Get the next image file name from the list.\n",
    "        image_name = self.images_list[idx]\n",
    "        # Create the path to image.\n",
    "        path_to_image = os.path.join(self.path_to_data, \"images\", image_name)\n",
    "        # Read image to a torch tensor. Since we are creating a new tensor here, we should move it to our preferred device.\n",
    "        image = read_image(path_to_image).to(device)\n",
    "        # Apply transformations.\n",
    "        transformed_image = self.augmentation(self.preprocessing(image))\n",
    "        # Pokemon name is the file name but without the extension .png, .jpg etc.\n",
    "        pokemon_name = image_name.split(\".\")[0]\n",
    "        # Find the pokemon from the csv and get its label code.\n",
    "        label = self.metadata[self.metadata[\"Name\"] == pokemon_name][\"label_code\"].to_numpy()\n",
    "        # Since we are creating a tensor here (torch.from_numpy), we should move it to our preferred device.\n",
    "        # tensor.squeeze() is a shaping operation that removes the dimensions with size 1.\n",
    "        label = torch.from_numpy(label).to(device).squeeze()\n",
    "        return transformed_image, label\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compulsory for iteration.\"\"\"\n",
    "        return len(self.images_list)\n",
    "    \n",
    "    # You can create other functions to simplify your code (__getitem__ function), such as get_label() or get_image() etc.\n",
    "    \n",
    "    def preprocessing(self, img: Tensor) -> Tensor:\n",
    "        # Optional\n",
    "        return img\n",
    "    \n",
    "    def augmentation(self, img: Tensor) -> Tensor:\n",
    "        # Optional\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Torch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader is a class that helps to iterate over your dataset, handles batching, data parallelization and various other functionalities. It has a default way to create batches, but needs an input Dataset class to iterate over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collating is a term that its functionality collects multiple samples (and its labels) to a batch and defines the batch-level Dataloader behaviour. Basically you can create a function that accepts batch_data (list of whatever is returned from __getitem function of your Dataset class). In this example, it is list of tuple of 2 tensors, these Tensors are the ones returned as (images, labels) from the __getitem function above. And the list of these actually multiple samples that you should create the batch from. Length of this list should be equal to your BATCH_SIZE. Collate function is usually provide more flexibility, especially when we need batches with different size each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataloaders.png](./dataloaders.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate\n",
    "from typing import List, Tuple # for type-annotations\n",
    "\n",
    "def my_batch_collate(batch_data: List[Tuple[Tensor, Tensor]]) -> Tensor:\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "PATH_TO_DATA = \"../sample_data/\"\n",
    "\n",
    "tr_dataset = PokemonDataset(PATH_TO_DATA)\n",
    "tr_dataloader = DataLoader(\n",
    "    tr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=default_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration should create a tensor of images in the following shape: [5, 4, 120, 120] which represents [batch_size, n_channels, img_height, img_width]. Note that RGB images have 3-channels, RGBA 4-channels and grayscale only one channel. Real data is usually very messy, thats why you will probably get errors most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out how the data is iterated !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image, label in tr_dataloader:\n",
    "    print(image.shape)\n",
    "    print(label.shape)\n",
    "\n",
    "    # Plot first image in the batch as grayscale (first channel).\n",
    "    # Note that we need to detach the gradients and move back to CPU to convert tensors to numpy !\n",
    "    plt.imshow(image.detach().cpu().numpy()[0, 0], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: As you have noticed, you cannot create a batch if there are images in the same batch with different color coding. You can handle this situation by padding for RGB and grayscale images to 4-channels, or you can also pick only one channel. This is a good example why we should use our own batch collation function for dataloading, or you can change this in Dataset class as well (suitable case for preprocessing function.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HINT__: Its easier when you use the torch functions: torch.stack() and torch.tensor.expand(). Check documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def img_channel_padding_collate(batch_data: List[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]:\n",
    "    # batch_data is a list of inputs returned from __getitem__ function of the Dataset class.\n",
    "    # Here you need to create one single tensor with size [batch_size, n_channels=4, img_height, img_width].\n",
    "\n",
    "    # Handle the situation here:\n",
    "    for img, label in batch_data:\n",
    "        print(img.shape, label)\n",
    "        \n",
    "    return # Return 2 tensors: batched_images, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader and attach your collate function.\n",
    "tr_dataloader = DataLoader(\n",
    "    tr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=img_channel_padding_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to run the iteration again:\n",
    "for image, label in tr_dataloader:\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
