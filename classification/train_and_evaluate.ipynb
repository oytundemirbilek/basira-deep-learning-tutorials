{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After becoming more familiar of how tensors work, we are ready to train our first model. In this tutorial, we will train an image classifier that detects hand-written digits. Therefore, there will be 10 classes, and we will be using a benchmark dataset MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3 # this equals to 0.001\n",
    "WEIGHT_DECAY = 1e-3 # this equals to 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(root=\"../mnist_tr/\", download=True, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a validation set, we can use the scikit-learn's train_test_split function that returns us the indices of both subsets, train and validation. Then we can create new datasets with the Subset class, later to pass into dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate indices: instead of the actual data we pass in integers instead\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(mnist_train)),\n",
    "    mnist_train.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# generate subset based on indices\n",
    "tr_dataset = Subset(mnist_train, train_indices)\n",
    "val_dataset = Subset(mnist_train, val_indices)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential, Linear, ReLU, Conv2d, MaxPool2d, AdaptiveMaxPool2d\n",
    "from torch import Tensor # Only for type-annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing a model, one of the most important concerns is that you should always keep track of the shape of the tensors at each step, each layer, each operation. When we add the first layer, we should consider the channel size of the image, here we need only one because the MNIST dataset is grayscale. Convolutional layers usually are independent of input size (height and width), but the fully connected layers are not. We can have a __size-independent__ classifier by adding an adaptive layer (AdaptiveMaxPool2d) before the fully connected layers, and we can use our model with varying size of images, or we can add spatial augmentations (i.e., affecting the input image size). This will also be the case when you have the model in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(Module):\n",
    "    def __init__(self, n_channels: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.convs = Sequential(\n",
    "            Conv2d(n_channels, 6, 5),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2, 2),\n",
    "            Conv2d(6, 16, 5),\n",
    "            ReLU(),\n",
    "            AdaptiveMaxPool2d(5),\n",
    "        )\n",
    "        self.fcs = Sequential(\n",
    "            Linear(16 * 5 * 5, 120), \n",
    "            ReLU(),\n",
    "            Linear(120, 84), \n",
    "            ReLU(),\n",
    "            Linear(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.convs(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fcs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training: optimizers and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets pick the device we can work with: CPU trainings might take a long time, so we should pick a GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\") # If on macos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define the model, the parameters (weights and biases) are initialized on your CPU. And PyTorch's \"Module\" class which we inherited from, has a .to() method to shift all parameters to GPU at once. This property is extremely useful, since you dont have to worry about shifting each parameter inside your model to GPU one-by-one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define the optimizer, we should pass the models parameters since it should have access to them to update them based on the attached gradients. You can also play with a few optimizer-related hyperparameters here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: PyTorch has 2 different implementations of Adam optimizers: Adam and AdamW. AdamW is weight decay implementation and Adam is L2-regularization implementation. Weight decay implementation is proven to be outperforming, for details you can check: https://arxiv.org/abs/1711.05101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model:\n",
    "model = ImageClassifier().to(DEVICE)\n",
    "\n",
    "# Define loss:\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer, you need to attach your model(s) parameters\n",
    "# to an optimizer object that will be responsible for updating your model.\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loops in PyTorch has a specific structure. First you need to get the loss, and this loss tensor should have only one value in it to be considered as a __\"root node\"__ of your gradient tree. Second, at some point before calling the backpropagation, gradients from any previous iteration/operation should be cleaned. Then, you can call .backward() method from the loss tensor which will automatically calculate all gradients at any point on the gradient tree all the way to the model parameters, __\"leaf nodes\"__ on the gradient tree. Lastly, after the gradients are obtained for the model parameters, optimizer can update the model parameters based on these gradients by calling the .step() method.\n",
    "\n",
    "SUMMARY: methods should be called in following order:\n",
    "1. optimizer.zero_grad()\n",
    "2. tr_loss.backward()\n",
    "3. optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 99999999999.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    tr_losses = []\n",
    "    for input_data, target_data in tr_dataloader:\n",
    "        # Dont forget to move your tensors to your device of preferrence !\n",
    "        input_data = input_data.to(DEVICE)\n",
    "        target_data = target_data.to(DEVICE)\n",
    "        pred_data = model(input_data)\n",
    "        tr_loss = loss_fn(pred_data, target_data)\n",
    "\n",
    "        # First step is to clear the calculated gradients from the previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss should be 1-value tensor and is a leaf in the gradient tree,\n",
    "        # Based on the gradient functions attached to the tensors (grad_fn), automatic\n",
    "        # backpropagation will calculate all gradients in the background.\n",
    "        tr_loss.backward()\n",
    "\n",
    "        # Optimizer object is responsible for updating model weights and biases,\n",
    "        # after the gradients are calculated.\n",
    "        optimizer.step()\n",
    "\n",
    "        # tensor.detach() function breaks the gradient tree and returns the tensor only including the data !\n",
    "        # Thats why you are only allowed to do that after you finish the gradient calculations.\n",
    "        tr_losses.append(tr_loss.detach())\n",
    "    \n",
    "    # tensor.item() returns the one and only attached value here: e.g., tensor((0.00135)) -> 0.00135\n",
    "    avg_tr_loss = torch.stack(tr_losses).mean().item()\n",
    "\n",
    "    # After a successful training iteration, we can run a validation loop to monitor model's performance:\n",
    "    # Since there will be no backpropagation on the validation step, it is unnecessary to store gradients.\n",
    "    # Therefore we use torch.no_grad to disable all gradient functions during validation for memory efficiency.\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for input_data, target_data in val_dataloader:\n",
    "            # Dont forget to move your tensors to your device of preferrence !\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            target_data = target_data.to(DEVICE)\n",
    "            pred_data = model(input_data)\n",
    "            val_loss = loss_fn(pred_data, target_data)\n",
    "            val_losses.append(val_loss.detach())\n",
    "        avg_val_loss = torch.stack(val_losses).mean().item()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{N_EPOCHS}, Avg.Tr.Loss: {avg_tr_loss}, Avg.Val.Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Save your model if the validation loss is better than before:\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"./digit_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial that your testing set is completely disjoint from your training and validation sets, plus it should have the correct representation of the use cases of your model in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect test results one by one (no-batch), because they will be used to calculate metrics.\n",
    "mnist_test = MNIST(root=\"../mnist_test/\", download=True, train=False, transform=ToTensor())\n",
    "\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing loop, we also do not need the gradients, and we can store the logits along with the predictions to use them in evaluation. Since logits represent the likelihood of belonging to a class, we can turn them into probabilities by scaling them between 0.0 - 1.0 with the softmax function. Then we can use them in ROC-AUC evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_logits = []\n",
    "    test_labels = []\n",
    "    for input_data, target_data in test_dataloader:\n",
    "        # Dont forget to move your tensors to your device of preferrence !\n",
    "        input_data = input_data.to(DEVICE)\n",
    "        target_data = target_data.to(DEVICE)\n",
    "        pred_data = model(input_data).squeeze()\n",
    "        test_logits.append(torch.softmax(pred_data, 0))\n",
    "        pred_class = torch.argmax(pred_data)\n",
    "        test_preds.append(pred_class)\n",
    "        test_labels.append(target_data)\n",
    "\n",
    "    logit_preds = torch.stack(test_logits).cpu().numpy()\n",
    "    class_preds = torch.stack(test_preds).cpu().numpy()\n",
    "    class_labels = torch.stack(test_labels).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the basic classification metrics: accuracy, f1-score and auc-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(class_labels, class_preds)\n",
    "f1 = f1_score(class_labels, class_preds, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(class_labels, logit_preds, multi_class=\"ovr\")\n",
    "acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the multi-class confusion matrix is crucial to see the model behaviour. We can also visualize our results for better interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(class_labels, class_preds)\n",
    "print(conf_mat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your model in production ! Dont forget to apply the preprocessing steps if you have added any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = read_image(\"./sample_draw_2.png\", mode=ImageReadMode.GRAY)\n",
    "    img = img.to(DEVICE).float().unsqueeze(0)\n",
    "    pred = model(img).squeeze()\n",
    "    pred_class = torch.argmax(pred)\n",
    "    print(f\"Predicted digit in the image: {pred_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is failing to correctly predict the digit. Since it is trained on prepared data with specific characteristics, it fails on different data. This is a very common problem in machine learning that training data domain does not align well with the real-life cases. This concept is widely recognized as __\"domain shift\"__. Think of possible reasons for that is happening, and possible solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot a sample from MNIST dataset, you will see that the data has fixed size (28, 28) and white drawing on black background, whereas the input image has a different size and is black drawing on white background. We can overcome these problems either by adding __augmentations__ on the training data, but for now lets try to add some quick fix to the test-time and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "with torch.no_grad():\n",
    "    resizer = Resize([28, 28])\n",
    "    img = read_image(\"../sample_draw_2.png\", mode=ImageReadMode.GRAY)\n",
    "    img = resizer(img)\n",
    "    # plt.imshow(img.squeeze().numpy())\n",
    "    # plt.show()\n",
    "    img = img.to(DEVICE).float().unsqueeze(0)\n",
    "    pred = model(img).squeeze()\n",
    "    pred_class = torch.argmax(pred)\n",
    "    print(f\"Predicted digit in the image: {pred_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this will not always be the case that you know why your predictions are not accurate, so you should always try to train models that can generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember you do not always have to do everything from scratch. There are many tools, repositories or Python packages to make machine learning easy. You can also use the template repository here for your future machine learning projects: https://github.com/oytundemirbilek/ml-template-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add augmentations to enable your model to predict digits accurately even if the input image size is \"too different\".\n",
    "\n",
    "2. Pick another loss function and report&compare results.\n",
    "\n",
    "3. Pick another optimizer and report&compare results.\n",
    "\n",
    "4. Switch your dataset to the PokemonDataset and try to train an image classifier that detects the type of pokemon from the input image (Type1 column in the table).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
