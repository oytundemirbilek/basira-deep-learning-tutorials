{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3 # this equals to 0.001\n",
    "WEIGHT_DECAY = 1e-3 # this equals to 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(root=\"../mnist_tr/\", download=True, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should define a validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate indices: instead of the actual data we pass in integers instead\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(mnist_train)),\n",
    "    mnist_train.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# generate subset based on indices\n",
    "tr_dataset = Subset(mnist_train, train_indices)\n",
    "val_dataset = Subset(mnist_train, val_indices)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential, Linear, ReLU, Conv2d, MaxPool2d, AdaptiveMaxPool2d\n",
    "from torch import Tensor # Only for type-annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers: BatchNorm, ReLU, Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(Module):\n",
    "    def __init__(self, n_channels: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.convs = Sequential(\n",
    "            Conv2d(n_channels, 6, 5),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2, 2),\n",
    "            Conv2d(6, 16, 5),\n",
    "            ReLU(),\n",
    "            AdaptiveMaxPool2d(5),\n",
    "        )\n",
    "        self.fcs = Sequential(\n",
    "            Linear(16 * 5 * 5, 120), \n",
    "            ReLU(),\n",
    "            Linear(120, 84), \n",
    "            ReLU(),\n",
    "            Linear(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.convs(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fcs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training: optimizers and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\") # If on macos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets pick an optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model:\n",
    "model = ImageClassifier().to(DEVICE)\n",
    "\n",
    "# Define loss:\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer, you need to attach your model(s) parameters\n",
    "# to an optimizer object that will be responsible for updating your model.\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch has 2 different Adam optimizers, Adam is ... and AdamW is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    tr_losses = []\n",
    "    for input_data, target_data in tr_dataloader:\n",
    "        # Dont forget to move your tensors to your device of preferrence !\n",
    "        input_data = input_data.to(DEVICE)\n",
    "        target_data = target_data.to(DEVICE)\n",
    "        pred_data = model(input_data)\n",
    "        tr_loss = loss_fn(pred_data, target_data)\n",
    "\n",
    "        # First step is to clear the calculated gradients from the previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss should be 1-value tensor and is a leaf in the gradient tree,\n",
    "        # Based on the gradient functions attached to the tensors (grad_fn), automatic\n",
    "        # backpropagation will calculate all gradients in the background.\n",
    "        tr_loss.backward()\n",
    "\n",
    "        # Optimizer object is responsible for updating model weights and biases,\n",
    "        # after the gradients are calculated.\n",
    "        optimizer.step()\n",
    "\n",
    "        # tensor.detach() function breaks the gradient tree and returns the tensor only including the data !\n",
    "        # Thats why you are only allowed to do that after you finish the gradient calculations.\n",
    "        tr_losses.append(tr_loss.detach())\n",
    "    \n",
    "    # tensor.item() returns the one and only attached value here: e.g., tensor((0.00135)) -> 0.00135\n",
    "    avg_tr_loss = torch.stack(tr_losses).mean().item()\n",
    "\n",
    "    # After a successful training iteration, we can run a validation loop to monitor model's performance:\n",
    "    # Since there will be no backpropagation on the validation step, it is unnecessary to store gradients.\n",
    "    # Therefore we use torch.no_grad to disable all gradient functions during validation for memory efficiency.\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for input_data, target_data in val_dataloader:\n",
    "            # Dont forget to move your tensors to your device of preferrence !\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            target_data = target_data.to(DEVICE)\n",
    "            pred_data = model(input_data)\n",
    "            val_loss = loss_fn(pred_data, target_data)\n",
    "            val_losses.append(val_loss.detach())\n",
    "        avg_val_loss = torch.stack(val_losses).mean().item()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{N_EPOCHS}, Avg.Tr.Loss: {avg_tr_loss}, Avg.Val.Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial that your testing set is completely disjoint from your training and validation sets, plus it should have the correct representation of the use cases of your model in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect test results one by one (no-batch), because they will be used to calculate metrics.\n",
    "mnist_test = MNIST(root=\"../mnist_test/\", download=True, train=False, transform=ToTensor())\n",
    "\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_logits = []\n",
    "    test_labels = []\n",
    "    for input_data, target_data in test_dataloader:\n",
    "        # Dont forget to move your tensors to your device of preferrence !\n",
    "        input_data = input_data.to(DEVICE)\n",
    "        target_data = target_data.to(DEVICE)\n",
    "        pred_data = model(input_data).squeeze()\n",
    "        test_logits.append(torch.softmax(pred_data, 0))\n",
    "        pred_class = torch.argmax(pred_data)\n",
    "        test_preds.append(pred_class)\n",
    "        test_labels.append(target_data)\n",
    "\n",
    "    logit_preds = torch.stack(test_logits).cpu().numpy()\n",
    "    class_preds = torch.stack(test_preds).cpu().numpy()\n",
    "    class_labels = torch.stack(test_labels).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here lets take a look at the basic classification metrics: accuracy and f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(class_labels, class_preds)\n",
    "f1 = f1_score(class_labels, class_preds, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(class_labels, logit_preds, multi_class=\"ovr\")\n",
    "acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to visualize our results for better interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can plot a ROC-AUC curve to investigate the model confidence on its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_curve(class_labels[np.where(class_labels==0)], logit_preds[np.where(class_labels==0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the multi-class confusion matrix is crucial to see the model behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(class_labels, class_preds)\n",
    "print(conf_mat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your model in production ! Dont forget to apply the preprocessing steps if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
